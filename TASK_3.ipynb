{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97117ca3-7385-4086-81a3-751c31823bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best parameters found:  {'feature_selection__k': 12, 'model': RandomForestClassifier(random_state=42), 'model__max_depth': 20, 'model__n_estimators': 50}\n",
      "Best score:  0.8394241417497232\n",
      "Test Accuracy:  0.7582417582417582\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.77        48\n",
      "           1       0.73      0.77      0.75        43\n",
      "\n",
      "    accuracy                           0.76        91\n",
      "   macro avg       0.76      0.76      0.76        91\n",
      "weighted avg       0.76      0.76      0.76        91\n",
      "\n",
      "ValueError: All arrays must be of the same length\n",
      "X_train shape: (212, 13)\n",
      "Features in X_train: Index(['Age', 'Sex', 'ChestPain', 'RestBP', 'Chol', 'Fbs', 'RestECG', 'MaxHR',\n",
      "       'ExAng', 'Oldpeak', 'Slope', 'Ca', 'Thal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Heart.csv\")\n",
    "\n",
    "# Drop the 'Unnamed: 0' column if it exists\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data = data.drop(columns='Unnamed: 0')\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns='AHD')\n",
    "y = data['AHD'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak', 'Ca']\n",
    "categorical_features = ['Sex', 'ChestPain', 'Fbs', 'ExAng', 'Slope', 'Thal']\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())  # Use MinMaxScaler to ensure non-negative values for chi2\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))  # Drop='first' to avoid dummy variable trap\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVC': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Create a pipeline that includes preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', SelectKBest(score_func=chi2)),\n",
    "    ('model', RandomForestClassifier(random_state=42))  # Default model\n",
    "])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'feature_selection__k': [10, 12, 13],  # k should not be more than number of features\n",
    "        'model': [RandomForestClassifier(random_state=42)],\n",
    "        'model__n_estimators': [50, 100],  # Applicable for RandomForest and GradientBoosting\n",
    "        'model__max_depth': [10, 20, None]  # Applicable for RandomForest and GradientBoosting\n",
    "    },\n",
    "    {\n",
    "        'feature_selection__k': [10, 12, 13],\n",
    "        'model': [GradientBoostingClassifier(random_state=42)],\n",
    "        'model__n_estimators': [50, 100],\n",
    "        'model__max_depth': [10, 20, None]\n",
    "    },\n",
    "    {\n",
    "        'feature_selection__k': [10, 12, 13],\n",
    "        'model': [SVC(random_state=42)],\n",
    "        'model__C': [0.1, 1, 10],  # Applicable for SVC\n",
    "        'model__kernel': ['linear', 'rbf']  # Applicable for SVC\n",
    "    }\n",
    "]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit grid search\n",
    "try:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print best parameters and best score\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy: \", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Analyze feature importance\n",
    "    if hasattr(grid_search.best_estimator_.named_steps['model'], 'feature_importances_'):\n",
    "        best_model = grid_search.best_estimator_.named_steps['model']\n",
    "        feature_importances = best_model.feature_importances_\n",
    "\n",
    "        # Get feature names\n",
    "        encoder = grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "        encoded_features = encoder.get_feature_names_out(categorical_features)\n",
    "        all_features = np.append(numeric_features, encoded_features)\n",
    "\n",
    "        # Create a DataFrame for feature importances\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': all_features,\n",
    "            'Importance': feature_importances\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        print(feature_importance_df)\n",
    "    else:\n",
    "        print(\"Best model does not support feature importances.\")\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "    # Additional debug info\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"Features in X_train: {X_train.columns}\")\n",
    "except AttributeError as e:\n",
    "    print(f\"AttributeError: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0779828-ed48-4ad4-8f16-542e76ec44b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
